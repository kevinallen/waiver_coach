{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nfldb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from ml.feature_extraction.nfldb_feature_extraction import WeeklyPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import LagPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import MeanPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import ExtractColumns\n",
    "from ml.feature_extraction.nfldb_feature_extraction import HandleNaN\n",
    "from ml.feature_extraction.nfldb_feature_extraction import FilterPlayedPercent\n",
    "from ml.nfldb_helpers.generic_helpers import week_player_id_list\n",
    "from ml.nfldb_helpers.generic_helpers import player_current_game_info\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split_index(n,test_size=0.2,seed=None):\n",
    "\tif(seed):\n",
    "\t\tnp.random.seed(seed)\n",
    "\trand_i = np.random.choice(range(n), n, replace=False)\n",
    "\ttest_i = rand_i[range(int(round(n*test_size)))]\n",
    "\ttrain_i = rand_i[range(int(round(n*test_size)),n)]\n",
    "\treturn train_i, test_i\n",
    "\n",
    "def split_by_year_week(X, test_yr_wk):\n",
    "\ttrain_i = []\n",
    "\ttest_i = []\n",
    "\tfor i in range(X.shape[0]):\n",
    "\t\tmatch = False\n",
    "\t\trow_yr_wk = (X.iloc[i]['year'], X.iloc[i]['week'])\n",
    "\t\tfor yr_wk in test_yr_wk:\n",
    "\t\t\tif row_yr_wk[0] == yr_wk[0] and row_yr_wk[1] == yr_wk[1]:\n",
    "\t\t\t\tmatch = True\n",
    "\t\t\t\ttest_i += [i]\n",
    "\t\t\t\tbreak\n",
    "\t\tif not match:\n",
    "\t\t\ttrain_i += [i]\n",
    "\treturn train_i, test_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to nfldb\n",
    "db = nfldb.connect()\n",
    "result_path = '../results'\n",
    "cache_path = '../data'\n",
    "position = 'RB'\n",
    "load_cached = True\n",
    "\n",
    "if(not load_cached):\n",
    "    # make player data transformer\n",
    "    yr_wk = [(j, i) for j in range(2009,2015) for i in range(1,18)]\n",
    "    yr_wk += [(2015, i) for i in range(1,7)]\n",
    "\n",
    "    #stats = ['rushing_yds','rushing_att']\n",
    "    stats = ['receiving_rec', 'receiving_tar', 'receiving_tds', 'receiving_yac_yds', 'receiving_yds', 'rushing_att', 'rushing_tds','rushing_yds']\n",
    "\n",
    "    player_info = ['player_id','full_name','position']\n",
    "    playerData = WeeklyPlayerData(db=db, yr_wk=yr_wk, stats=stats, player_info=player_info, fill_time=True, position=position)\n",
    "\n",
    "    # creates lags of the data\n",
    "    lag_cols = ['year', 'week', 'played'] + stats\n",
    "    nlag = 6\n",
    "    lagData = LagPlayerData(nlag=nlag, groupby_cols=['player_id'], lag_cols=lag_cols, same_year_bool=True)\n",
    "\n",
    "    # creates means of the data\n",
    "    mean_cols = stats\n",
    "    meanData = MeanPlayerData(groupby_cols=['player_id'], mean_cols=mean_cols)\n",
    "\n",
    "    # pipeline for getting data\n",
    "    pipe1 = Pipeline(steps=[('data',playerData), ('lag',lagData), ('mean',meanData)])\n",
    "    #processed_data = pipe1.fit_transform(X=None)\n",
    "\n",
    "    # print processed_data\n",
    "    # pipeline for seting which columns we want and handling NaN\n",
    "    pct_played_threshold = 0.0\n",
    "    pipe2_steps = [('handle',HandleNaN(method='fill')), ('filterplayed',FilterPlayedPercent(pct_played_threshold=pct_played_threshold))]\n",
    "    pipe2 = Pipeline(steps=pipe2_steps)\n",
    "\n",
    "    pipe = Pipeline([('pipe1',pipe1),('pipe2',pipe2)])\n",
    "\n",
    "    all_columns = pipe.fit_transform(X=None)\n",
    "    all_columns.position = all_columns.position.astype(str)\n",
    "\n",
    "    # pickle files\n",
    "    pickle.dump(pipe.set_params(pipe1__data__db=None), open(cache_path + '/pipe_'+position+'.p', 'wb'))\n",
    "    pickle.dump(all_columns, open(cache_path + '/data_'+position+'.p', 'wb'))\n",
    "else:\n",
    "    # Load from \"cached\" (pickled) transformer and data\n",
    "    # data\n",
    "    all_columns = pickle.load(open(cache_path + '/data_'+position+'.p', 'rb'))\n",
    "    # pipeline\n",
    "    pipe = pickle.load(open(cache_path + '/pipe_'+position+'.p', 'rb'))\n",
    "    # retrieve the list of stats that was predicted\n",
    "    pipe_params = pipe.get_params()\n",
    "    stats = pipe_params['pipe1__data__stats']\n",
    "\n",
    "\n",
    "pipe.set_params(pipe1__data__db=db)\n",
    "\n",
    "full_train = all_columns\n",
    "\n",
    "# picks columns to model\n",
    "lag_cols = [stat + '_lag' for stat in stats]\n",
    "mean_cols = [stat + '_mean' for stat in stats]\n",
    "other_cols = ['same_year_lag', 'played_lag']\n",
    "\n",
    "infoColumns = ExtractColumns(like=[], exact=['year','week','time','player_id','full_name'])\n",
    "row_info = infoColumns.fit_transform(X=full_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### prediction data\n",
    "# prediction pipeline\n",
    "pred_data_pipe = pipe#Pipeline(steps=[('pipe1',pipe1),('pipe2',pipe2)])\n",
    "\n",
    "# get information we need to make predictions\n",
    "season_phase, cur_year, cur_week = nfldb.current(db)\n",
    "pred_week = cur_week + 1\t\n",
    "pred_yr_wk = [(j, i) for j in range(2009,cur_year-1) for i in range(1,18)]\n",
    "pred_yr_wk += [(cur_year, i) for i in range(1,pred_week+1)]\n",
    "\n",
    "pred_data_pipe.set_params(pipe1__data__yr_wk = pred_yr_wk)\n",
    "\n",
    "player_ids = week_player_id_list(db, cur_year, pred_week, position='RB')\n",
    "#player_ids = player_ids[0:2] + player_ids[-3:-1]\n",
    "\n",
    "pred_data = pred_data_pipe.fit_transform(player_ids)\n",
    "pred_info = infoColumns.fit_transform(X=pred_data)\n",
    "\n",
    "# get extra info like team and opponent\n",
    "# should probably be put in to infoColumns transformer later\n",
    "extra_info = player_current_game_info(db, year=cur_year, week=pred_week, player_ids = list(pred_info['player_id']))\n",
    "join_on = ['player_id']\n",
    "add_on = ['team', 'opp_team', 'at_home']\n",
    "pred_info = pred_info.join(extra_info.set_index(join_on).loc[:,add_on], on=join_on)\n",
    "\n",
    "# predict for the last week\n",
    "pred_yr_wk_t = [pred_yr_wk[-1]]\n",
    "garbage_i, predict_i = split_by_year_week(pred_data, pred_yr_wk_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "df_pred = pred_info.iloc[predict_i]\n",
    "\n",
    "# set y_col\n",
    "#y_cols = ['played', 'receiving_rec', 'receiving_tds', 'receiving_yds', 'rushing_att', 'rushing_tds','rushing_yds']\n",
    "y_col = 'played'\n",
    "#y_cols = ['rushing_yds', 'played']\n",
    "\n",
    "played_only = True\n",
    "\n",
    "# Pick the right columns\n",
    "keep_like = [y_col] + lag_cols + mean_cols + other_cols\n",
    "pickColumns = ExtractColumns(like=keep_like)\n",
    "X_y = pickColumns.fit_transform(X=full_train)\n",
    "\n",
    "if(played_only and y_col != 'played' and 'played' in X_y.columns):\n",
    "    played_bool = X_y['played'] == 1\n",
    "    X_y = X_y[played_bool]\n",
    "\n",
    "# get X and y\n",
    "y = X_y[y_col]\n",
    "X = X_y.drop(y_col, axis=1)\n",
    "\n",
    "# random split train and test\n",
    "train_i, test_i = train_test_split_index(X.shape[0], test_size=0.1, seed=0)\n",
    "# set up data\n",
    "y_train = y.iloc[train_i]\n",
    "y_test = y.iloc[test_i]\n",
    "X_train = X.iloc[train_i]\n",
    "X_test = X.iloc[test_i]\n",
    "### Test Predictions\n",
    "\n",
    "if(y_col == 'played'):\n",
    "    # Gradident Boosting\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1).fit(X_train, y_train)\n",
    "    # Random Forest Regressor\n",
    "    rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "    # Linear Regression\n",
    "    lr = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82107697,  0.88794581,  0.79349091,  0.89515036,  0.84302234,\n",
       "        0.31224123,  0.83670637,  0.92919044,  0.88209891,  0.89769976,\n",
       "        0.90884519,  0.23685376,  0.91799905,  0.63638529,  0.35981484,\n",
       "        0.83958061,  0.86533922,  0.66128134,  0.49636513,  0.97038593,\n",
       "        0.89203158,  0.9372238 ,  0.92231371,  0.83989459,  0.19388495,\n",
       "        0.90688326,  0.34871683,  0.83152645,  0.52131849,  0.38560038,\n",
       "        0.41949462,  0.4563776 ,  0.91096455,  0.8834371 ,  0.67946019,\n",
       "        0.88532775,  0.31189416,  0.95616208,  0.90066937,  0.48845621,\n",
       "        0.62498908,  0.31224123,  0.31685558,  0.2052282 ,  0.50589548,\n",
       "        0.33636791,  0.73383069,  0.73258795,  0.2067679 ,  0.90275994,\n",
       "        0.89724364,  0.73186614,  0.89009656,  0.85937199,  0.73139111,\n",
       "        0.22248197,  0.41308815,  0.21932525,  0.95197178,  0.74558564,\n",
       "        0.65528501,  0.85922566,  0.41041426,  0.86110192,  0.69542188,\n",
       "        0.73724934,  0.23103807,  0.23851364,  0.91142947,  0.85114454,\n",
       "        0.72153729,  0.91525887,  0.19325197,  0.57098652,  0.76201832,\n",
       "        0.85567175,  0.86387694,  0.39521446,  0.92838825,  0.91727677,\n",
       "        0.70921132,  0.91389632,  0.38887638,  0.70787077,  0.68672049,\n",
       "        0.80504974,  0.80599784,  0.89627015,  0.76840583,  0.91706643,\n",
       "        0.67643703,  0.23000352,  0.93395932,  0.8992929 ,  0.41631199,\n",
       "        0.54295503,  0.85680105,  0.83521987,  0.89917424,  0.91621582,\n",
       "        0.18678472,  0.71859532,  0.87355165,  0.85926511,  0.93016677,\n",
       "        0.86462037,  0.60725159,  0.93552679,  0.78550592,  0.52819512,\n",
       "        0.96571898,  0.83784764,  0.87048206,  0.25970435,  0.87857302,\n",
       "        0.82456993,  0.77950076,  0.7221721 ,  0.81405864,  0.70110118,\n",
       "        0.89294861,  0.89156226,  0.79547016,  0.71728147,  0.83138592,\n",
       "        0.84656333,  0.89709289,  0.92973727,  0.95644943,  0.95715927,\n",
       "        0.53437029,  0.80794306,  0.81027271,  0.87234118,  0.39791575,\n",
       "        0.49859989,  0.68262641,  0.86004658,  0.24908198,  0.34879178,\n",
       "        0.81732654,  0.90249835,  0.62791387,  0.86939692,  0.91323279,\n",
       "        0.54742427,  0.41518429,  0.74951008,  0.87694091,  0.45070943,\n",
       "        0.80387435,  0.89778426,  0.62881905,  0.87953281,  0.93008228,\n",
       "        0.90725078,  0.85916233,  0.91675181,  0.77063362,  0.83967941,\n",
       "        0.92813534,  0.90193337,  0.84204739,  0.95978917,  0.58363089,\n",
       "        0.8393847 ,  0.87495709,  0.9387203 ,  0.83424709,  0.84941552,\n",
       "        0.84011908,  0.25970435,  0.9357921 ,  0.89333522,  0.81908037,\n",
       "        0.58405605,  0.86026192,  0.94197919,  0.60632896,  0.41240785,\n",
       "        0.91248611,  0.9247011 ,  0.95158848,  0.77753984,  0.71469448,\n",
       "        0.80898641,  0.19817485,  0.92886928,  0.2052282 ,  0.84837835,\n",
       "        0.77760187,  0.57233468,  0.8303452 ,  0.97626158,  0.86596693,\n",
       "        0.9284798 ,  0.766215  ,  0.91702087,  0.89580889,  0.89307602,\n",
       "        0.6035891 ,  0.2067679 ,  0.67061179,  0.95535578,  0.46547445,\n",
       "        0.35014884,  0.90601689,  0.34254177,  0.88771615,  0.85969148,\n",
       "        0.83477325,  0.23513934,  0.82172883,  0.89822564,  0.83696281,\n",
       "        0.58187473,  0.94422447,  0.31189416,  0.33277997,  0.89794918,\n",
       "        0.48411815,  0.93542497,  0.94896673,  0.814696  ,  0.89503378,\n",
       "        0.84035866,  0.85208353,  0.86762824,  0.89579053,  0.8220266 ,\n",
       "        0.93179053,  0.97056936,  0.84499636,  0.7216489 ,  0.790075  ,\n",
       "        0.95497409,  0.94580844,  0.72677021,  0.87957049,  0.57585303,\n",
       "        0.86517352,  0.38529551,  0.88116611,  0.4598324 ,  0.89665327,\n",
       "        0.74558564,  0.91686114,  0.25286554,  0.85429101,  0.54295503,\n",
       "        0.74275603,  0.96136946,  0.89857636,  0.95822572,  0.94526622,\n",
       "        0.8703629 ,  0.82545612,  0.8994169 ,  0.85312532,  0.82769785,\n",
       "        0.89108704,  0.79981915,  0.82999097,  0.90860531,  0.89589356,\n",
       "        0.88681071,  0.83107081,  0.91084204,  0.5447076 ,  0.7534357 ,\n",
       "        0.94618953,  0.38960115,  0.79204722,  0.34967211,  0.82826117,\n",
       "        0.46175596,  0.87670278,  0.84997529,  0.87260751,  0.9038665 ,\n",
       "        0.77330193,  0.90464552,  0.40203265,  0.94971422,  0.52131849,\n",
       "        0.65339719,  0.9503451 ,  0.94301704,  0.86205369,  0.89937146,\n",
       "        0.64110803,  0.61550655,  0.49843235,  0.31189416,  0.25276538,\n",
       "        0.9071445 ,  0.92858271,  0.82239875,  0.56622789,  0.81859076,\n",
       "        0.37561413,  0.72658308,  0.84598328,  0.87000262,  0.78802834,\n",
       "        0.8841523 ,  0.50155276,  0.88492631,  0.87522058,  0.82141168,\n",
       "        0.80073659,  0.77148098,  0.90811081,  0.89334794,  0.64278244,\n",
       "        0.87580524,  0.70452999,  0.39017892,  0.27507837,  0.87440929,\n",
       "        0.66678198,  0.88399572,  0.83582373,  0.3207407 ,  0.90179319,\n",
       "        0.92558477,  0.8169103 ,  0.95204198,  0.78502078,  0.81260687,\n",
       "        0.76825163,  0.54908369,  0.62960022,  0.95130531,  0.41442402,\n",
       "        0.96239562,  0.88006394,  0.86992119,  0.92363888,  0.78026481,\n",
       "        0.54719396,  0.3095765 ,  0.29915402,  0.56017049,  0.807357  ,\n",
       "        0.87168533,  0.72577121,  0.94848661,  0.75160005,  0.9510609 ,\n",
       "        0.88359641,  0.88863045,  0.82758002,  0.59032033,  0.89695788,\n",
       "        0.82248891,  0.54719396,  0.2052282 ,  0.94151496,  0.84856908,\n",
       "        0.85616349,  0.69642049,  0.48411362,  0.78377368,  0.93996674,\n",
       "        0.2281604 ,  0.43553434,  0.68203456,  0.96288004,  0.78424095,\n",
       "        0.32372437,  0.2052282 ,  0.71218742,  0.39086647,  0.88907599,\n",
       "        0.9185559 ,  0.9098686 ,  0.91380701,  0.39508299,  0.65235186,\n",
       "        0.89550332,  0.37730444,  0.96246719,  0.71041204,  0.31874157,\n",
       "        0.88281102,  0.87173092,  0.29957288,  0.90380373,  0.78918493,\n",
       "        0.67845888,  0.8950059 ,  0.53725542,  0.35349581,  0.90893609,\n",
       "        0.65861578,  0.86179879,  0.67747562,  0.86348925,  0.85789887,\n",
       "        0.91776527,  0.88883479,  0.22318104,  0.56586421,  0.90348374,\n",
       "        0.72317894,  0.95856635,  0.80081072,  0.41469089,  0.54952533,\n",
       "        0.37516638,  0.31224123,  0.9530468 ,  0.8026106 ,  0.74048706,\n",
       "        0.90858718,  0.73680985,  0.2052282 ,  0.91406407,  0.23129495,\n",
       "        0.2171237 ,  0.85685925])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting played\n",
      "Gradient Boosting: RMSE 0.36 | MAE 0.27\n",
      "Random Forest: RMSE 0.36 | MAE 0.27\n",
      "Logistic Regression: RMSE 0.37 | MAE 0.28\n",
      "Predicting receiving_rec\n",
      "Gradient Boosting: RMSE 1.42 | MAE 0.99\n",
      "Random Forest: RMSE 1.45 | MAE 0.98\n",
      "Linear Regression: RMSE 1.43 | MAE 1.00\n",
      "Predicting rushing_yds\n",
      "Gradient Boosting: RMSE 30.32 | MAE 20.68\n",
      "Random Forest: RMSE 31.21 | MAE 21.00\n",
      "Linear Regression: RMSE 29.76 | MAE 20.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_pred = pred_info.iloc[predict_i]\n",
    "\n",
    "# set y_col\n",
    "#y_cols = ['played', 'receiving_rec', 'receiving_tds', 'receiving_yds', 'rushing_att', 'rushing_tds','rushing_yds']\n",
    "y_cols = ['played', 'receiving_rec', 'rushing_yds']\n",
    "#y_cols = ['rushing_yds', 'played']\n",
    "\n",
    "played_only = True\n",
    "\n",
    "\n",
    "for y_col in y_cols:\n",
    "    # Pick the right columns\n",
    "    keep_like = [y_col] + lag_cols + mean_cols + other_cols\n",
    "    pickColumns = ExtractColumns(like=keep_like)\n",
    "    X_y = pickColumns.fit_transform(X=full_train)\n",
    "\n",
    "    if(played_only and y_col != 'played' and 'played' in X_y.columns):\n",
    "        played_bool = X_y['played'] == 1\n",
    "        X_y = X_y[played_bool]\n",
    "\n",
    "    # get X and y\n",
    "    y = X_y[y_col]\n",
    "    X = X_y.drop(y_col, axis=1)\n",
    "\n",
    "    # random split train and test\n",
    "    train_i, test_i = train_test_split_index(X.shape[0], test_size=0.1, seed=0)\n",
    "    # set up data\n",
    "    y_train = y.iloc[train_i]\n",
    "    y_test = y.iloc[test_i]\n",
    "    X_train = X.iloc[train_i]\n",
    "    X_test = X.iloc[test_i]\n",
    "    ### Test Predictions\n",
    "\n",
    "    if(y_col == 'played'):\n",
    "        # Gradident Boosting\n",
    "        gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1).fit(X_train, y_train)\n",
    "        gb_test = gb.predict_proba(X_test)[:,1]\n",
    "        # Random Forest Regressor\n",
    "        rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "        rf_test = rf.predict_proba(X_test)[:,1]\n",
    "        # Logistic Regression\n",
    "        lr = LogisticRegression().fit(X_train, y_train)\n",
    "        lr_test = lr.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        # Gradident Boosting\n",
    "        gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1).fit(X_train, y_train)\n",
    "        gb_test = gb.predict(X_test)\n",
    "        # Random Forest Regressor\n",
    "        rf = RandomForestRegressor().fit(X_train, y_train)\n",
    "        rf_test = rf.predict(X_test)\n",
    "        # Linear Regression\n",
    "        lr = LinearRegression().fit(X_train, y_train)\n",
    "        lr_test = lr.predict(X_test)\n",
    "\n",
    "    gb_rmse = mean_squared_error(y_test, gb_test)**0.5\n",
    "    gb_mae = mean_absolute_error(y_test, gb_test)\n",
    "\n",
    "    predict_test = rf.predict(X_test)\n",
    "    rf_rmse = mean_squared_error(y_test, rf_test)**0.5\n",
    "    rf_mae = mean_absolute_error(y_test, rf_test)\n",
    "\n",
    "    predict_test = lr.predict(X_test)\n",
    "    lr_rmse = mean_squared_error(y_test, lr_test)**0.5\n",
    "    lr_mae = mean_absolute_error(y_test, lr_test)\n",
    "    # Print Results\n",
    "    print 'Predicting %s' % (y_col)\n",
    "    print 'Gradient Boosting: RMSE %.2f | MAE %.2f' % (gb_rmse, gb_mae)\n",
    "    print 'Random Forest: RMSE %.2f | MAE %.2f' % (rf_rmse, rf_mae)\n",
    "    if(y_col == 'played'):\n",
    "        print 'Logistic Regression: RMSE %.2f | MAE %.2f' % (lr_rmse, lr_mae)\n",
    "    else:\n",
    "        print 'Linear Regression: RMSE %.2f | MAE %.2f' % (lr_rmse, lr_mae)\n",
    "    # Build full models on all data\n",
    "\n",
    "    gb = gb.fit(X, y)\n",
    "    rf = rf.fit(X, y)\n",
    "    lr = lr.fit(X, y)\n",
    "    #### Next week's predictions\n",
    "    #pipe1 = pipe1.set_params(data__yr_wk=yr_wk_pred)\n",
    "    #data1 = pipe1.transform(X=None)\n",
    "    #data2 = pipe2.transform(X=data1)\n",
    "    #X_y_pred = pickColumns.transform(X=data2)\n",
    "    #info_pred = infoColumns.transform(X=data2)\n",
    "    #X_pred = X_y_pred.drop(y_col, axis=1)\n",
    "    #y_pred = X_y_pred[y_col]\n",
    "    # Make prediction, just gbr for now\n",
    "    X_pred = pickColumns.fit_transform(X=pred_data).drop(y_col, axis=1)\n",
    "    \n",
    "    if(y_col == 'played'):\n",
    "        preds = gb.predict_proba(X_pred.iloc[predict_i])[:,1]\n",
    "    else:\n",
    "        preds = gb.predict(X_pred.iloc[predict_i])\n",
    "\n",
    "    df_pred[y_col] = preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
