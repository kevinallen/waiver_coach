{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nfldb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from ml.feature_extraction.nfldb_feature_extraction import WeeklyPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import LagPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import MeanPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import ExtractColumns\n",
    "from ml.feature_extraction.nfldb_feature_extraction import HandleNaN\n",
    "from ml.feature_extraction.nfldb_feature_extraction import FilterPlayedPercent\n",
    "from ml.helpers.nfldb_helpers import week_player_id_list\n",
    "from ml.helpers.nfldb_helpers import player_current_game_info\n",
    "from ml.helpers.scoring_helpers import make_scorer\n",
    "from ml.helpers.scoring_helpers import score_stats\n",
    "from ml.helpers.testing_helpers import train_test_split_index\n",
    "from ml.helpers.testing_helpers import split_by_year_week\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split_index(n,test_size=0.2,seed=None):\n",
    "\tif(seed):\n",
    "\t\tnp.random.seed(seed)\n",
    "\trand_i = np.random.choice(range(n), n, replace=False)\n",
    "\ttest_i = rand_i[range(int(round(n*test_size)))]\n",
    "\ttrain_i = rand_i[range(int(round(n*test_size)),n)]\n",
    "\treturn train_i, test_i\n",
    "\n",
    "def split_by_year_week(X, test_yr_wk):\n",
    "\ttrain_i = []\n",
    "\ttest_i = []\n",
    "\tfor i in range(X.shape[0]):\n",
    "\t\tmatch = False\n",
    "\t\trow_yr_wk = (X.iloc[i]['year'], X.iloc[i]['week'])\n",
    "\t\tfor yr_wk in test_yr_wk:\n",
    "\t\t\tif row_yr_wk[0] == yr_wk[0] and row_yr_wk[1] == yr_wk[1]:\n",
    "\t\t\t\tmatch = True\n",
    "\t\t\t\ttest_i += [i]\n",
    "\t\t\t\tbreak\n",
    "\t\tif not match:\n",
    "\t\t\ttrain_i += [i]\n",
    "\treturn train_i, test_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to nfldb\n",
    "db = nfldb.connect()\n",
    "result_path = '../results'\n",
    "cache_path = '../data'\n",
    "position = 'RB'\n",
    "load_cached = True\n",
    "\n",
    "if(not load_cached):\n",
    "    # make player data transformer\n",
    "    yr_wk = [(j, i) for j in range(2009,2015) for i in range(1,18)]\n",
    "    yr_wk += [(2015, i) for i in range(1,7)]\n",
    "\n",
    "    #stats = ['rushing_yds','rushing_att']\n",
    "    stats = ['receiving_rec', 'receiving_tar', 'receiving_tds', 'receiving_yac_yds', 'receiving_yds', 'rushing_att', 'rushing_tds','rushing_yds']\n",
    "\n",
    "    player_info = ['player_id','full_name','position']\n",
    "    playerData = WeeklyPlayerData(db=db, yr_wk=yr_wk, stats=stats, player_info=player_info, fill_time=True, position=position)\n",
    "\n",
    "    # creates lags of the data\n",
    "    lag_cols = ['year', 'week', 'played'] + stats\n",
    "    nlag = 6\n",
    "    lagData = LagPlayerData(nlag=nlag, groupby_cols=['player_id'], lag_cols=lag_cols, same_year_bool=True)\n",
    "\n",
    "    # creates means of the data\n",
    "    mean_cols = stats\n",
    "    meanData = MeanPlayerData(groupby_cols=['player_id'], mean_cols=mean_cols)\n",
    "\n",
    "    # pipeline for getting data\n",
    "    pipe1 = Pipeline(steps=[('data',playerData), ('lag',lagData), ('mean',meanData)])\n",
    "    #processed_data = pipe1.fit_transform(X=None)\n",
    "\n",
    "    # print processed_data\n",
    "    # pipeline for seting which columns we want and handling NaN\n",
    "    pct_played_threshold = 0.0\n",
    "    pipe2_steps = [('handle',HandleNaN(method='fill')), ('filterplayed',FilterPlayedPercent(pct_played_threshold=pct_played_threshold))]\n",
    "    pipe2 = Pipeline(steps=pipe2_steps)\n",
    "\n",
    "    pipe = Pipeline([('pipe1',pipe1),('pipe2',pipe2)])\n",
    "\n",
    "    all_columns = pipe.fit_transform(X=None)\n",
    "    all_columns.position = all_columns.position.astype(str)\n",
    "\n",
    "    # pickle files\n",
    "    pickle.dump(pipe.set_params(pipe1__data__db=None), open(cache_path + '/pipe_'+position+'.p', 'wb'))\n",
    "    pickle.dump(all_columns, open(cache_path + '/data_'+position+'.p', 'wb'))\n",
    "else:\n",
    "    # Load from \"cached\" (pickled) transformer and data\n",
    "    # data\n",
    "    all_columns = pickle.load(open(cache_path + '/data_'+position+'.p', 'rb'))\n",
    "    # pipeline\n",
    "    pipe = pickle.load(open(cache_path + '/pipe_'+position+'.p', 'rb'))\n",
    "    # retrieve the list of stats that was predicted\n",
    "    pipe_params = pipe.get_params()\n",
    "    stats = pipe_params['pipe1__data__stats']\n",
    "\n",
    "\n",
    "pipe.set_params(pipe1__data__db=db)\n",
    "\n",
    "full_train = all_columns\n",
    "\n",
    "# picks columns to model\n",
    "lag_cols = [stat + '_lag' for stat in stats]\n",
    "mean_cols = [stat + '_mean' for stat in stats]\n",
    "other_cols = ['same_year_lag', 'played_lag']\n",
    "\n",
    "infoColumns = ExtractColumns(like=[], exact=['year','week','time','player_id','full_name'])\n",
    "row_info = infoColumns.fit_transform(X=full_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### prediction data\n",
    "# prediction pipeline\n",
    "pred_data_pipe = pipe#Pipeline(steps=[('pipe1',pipe1),('pipe2',pipe2)])\n",
    "\n",
    "# get information we need to make predictions\n",
    "season_phase, cur_year, cur_week = nfldb.current(db)\n",
    "pred_week = cur_week + 1\t\n",
    "pred_yr_wk = [(j, i) for j in range(2009,cur_year-1) for i in range(1,18)]\n",
    "pred_yr_wk += [(cur_year, i) for i in range(1,pred_week+1)]\n",
    "\n",
    "pred_data_pipe.set_params(pipe1__data__yr_wk = pred_yr_wk)\n",
    "\n",
    "player_ids = week_player_id_list(db, cur_year, pred_week, position='RB')\n",
    "#player_ids = player_ids[0:2] + player_ids[-3:-1]\n",
    "\n",
    "pred_data = pred_data_pipe.fit_transform(player_ids)\n",
    "pred_info = infoColumns.fit_transform(X=pred_data)\n",
    "\n",
    "# get extra info like team and opponent\n",
    "# should probably be put in to infoColumns transformer later\n",
    "extra_info = player_current_game_info(db, year=cur_year, week=pred_week, player_ids = list(pred_info['player_id']))\n",
    "join_on = ['player_id']\n",
    "add_on = ['team', 'opp_team', 'at_home']\n",
    "pred_info = pred_info.join(extra_info.set_index(join_on).loc[:,add_on], on=join_on)\n",
    "\n",
    "# predict for the last week\n",
    "pred_yr_wk_t = [pred_yr_wk[-1]]\n",
    "garbage_i, predict_i = split_by_year_week(pred_data, pred_yr_wk_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "df_pred = pred_info.iloc[predict_i]\n",
    "\n",
    "# set y_col\n",
    "#y_cols = ['played', 'receiving_rec', 'receiving_tds', 'receiving_yds', 'rushing_att', 'rushing_tds','rushing_yds']\n",
    "y_col = 'played'\n",
    "#y_cols = ['rushing_yds', 'played']\n",
    "\n",
    "played_only = True\n",
    "\n",
    "# Pick the right columns\n",
    "keep_like = [y_col] + lag_cols + mean_cols + other_cols\n",
    "pickColumns = ExtractColumns(like=keep_like)\n",
    "X_y = pickColumns.fit_transform(X=full_train)\n",
    "\n",
    "if(played_only and y_col != 'played' and 'played' in X_y.columns):\n",
    "    played_bool = X_y['played'] == 1\n",
    "    X_y = X_y[played_bool]\n",
    "\n",
    "# get X and y\n",
    "y = X_y[y_col]\n",
    "X = X_y.drop(y_col, axis=1)\n",
    "\n",
    "# random split train and test\n",
    "train_i, test_i = train_test_split_index(X.shape[0], test_size=0.1, seed=0)\n",
    "# set up data\n",
    "y_train = y.iloc[train_i]\n",
    "y_test = y.iloc[test_i]\n",
    "X_train = X.iloc[train_i]\n",
    "X_test = X.iloc[test_i]\n",
    "### Test Predictions\n",
    "\n",
    "if(y_col == 'played'):\n",
    "    # Gradident Boosting\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1).fit(X_train, y_train)\n",
    "    # Random Forest Regressor\n",
    "    rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "    # Linear Regression\n",
    "    lr = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pred = pred_info.iloc[predict_i]\n",
    "\n",
    "# set y_col\n",
    "#y_cols = ['played', 'receiving_rec', 'receiving_tds', 'receiving_yds', 'rushing_att', 'rushing_tds','rushing_yds']\n",
    "y_cols = ['played', 'receiving_rec', 'rushing_yds']\n",
    "#y_cols = ['rushing_yds', 'played']\n",
    "\n",
    "played_only = True\n",
    "\n",
    "\n",
    "for y_col in y_cols:\n",
    "    # Pick the right columns\n",
    "    keep_like = [y_col] + lag_cols + mean_cols + other_cols\n",
    "    pickColumns = ExtractColumns(like=keep_like)\n",
    "    X_y = pickColumns.fit_transform(X=full_train)\n",
    "\n",
    "    if(played_only and y_col != 'played' and 'played' in X_y.columns):\n",
    "        played_bool = X_y['played'] == 1\n",
    "        X_y = X_y[played_bool]\n",
    "\n",
    "    # get X and y\n",
    "    y = X_y[y_col]\n",
    "    X = X_y.drop(y_col, axis=1)\n",
    "\n",
    "    # random split train and test\n",
    "    train_i, test_i = train_test_split_index(X.shape[0], test_size=0.1, seed=0)\n",
    "    # set up data\n",
    "    y_train = y.iloc[train_i]\n",
    "    y_test = y.iloc[test_i]\n",
    "    X_train = X.iloc[train_i]\n",
    "    X_test = X.iloc[test_i]\n",
    "    ### Test Predictions\n",
    "\n",
    "    if(y_col == 'played'):\n",
    "        # Gradident Boosting\n",
    "        gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1).fit(X_train, y_train)\n",
    "        gb_test = gb.predict_proba(X_test)[:,1]\n",
    "        # Random Forest Regressor\n",
    "        rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "        rf_test = rf.predict_proba(X_test)[:,1]\n",
    "        # Logistic Regression\n",
    "        lr = LogisticRegression().fit(X_train, y_train)\n",
    "        lr_test = lr.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        # Gradident Boosting\n",
    "        gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1).fit(X_train, y_train)\n",
    "        gb_test = gb.predict(X_test)\n",
    "        # Random Forest Regressor\n",
    "        rf = RandomForestRegressor().fit(X_train, y_train)\n",
    "        rf_test = rf.predict(X_test)\n",
    "        # Linear Regression\n",
    "        lr = LinearRegression().fit(X_train, y_train)\n",
    "        lr_test = lr.predict(X_test)\n",
    "\n",
    "    gb_rmse = mean_squared_error(y_test, gb_test)**0.5\n",
    "    gb_mae = mean_absolute_error(y_test, gb_test)\n",
    "\n",
    "    predict_test = rf.predict(X_test)\n",
    "    rf_rmse = mean_squared_error(y_test, rf_test)**0.5\n",
    "    rf_mae = mean_absolute_error(y_test, rf_test)\n",
    "\n",
    "    predict_test = lr.predict(X_test)\n",
    "    lr_rmse = mean_squared_error(y_test, lr_test)**0.5\n",
    "    lr_mae = mean_absolute_error(y_test, lr_test)\n",
    "    # Print Results\n",
    "    print 'Predicting %s' % (y_col)\n",
    "    print 'Gradient Boosting: RMSE %.2f | MAE %.2f' % (gb_rmse, gb_mae)\n",
    "    print 'Random Forest: RMSE %.2f | MAE %.2f' % (rf_rmse, rf_mae)\n",
    "    if(y_col == 'played'):\n",
    "        print 'Logistic Regression: RMSE %.2f | MAE %.2f' % (lr_rmse, lr_mae)\n",
    "    else:\n",
    "        print 'Linear Regression: RMSE %.2f | MAE %.2f' % (lr_rmse, lr_mae)\n",
    "    # Build full models on all data\n",
    "\n",
    "    gb = gb.fit(X, y)\n",
    "    rf = rf.fit(X, y)\n",
    "    lr = lr.fit(X, y)\n",
    "    #### Next week's predictions\n",
    "    #pipe1 = pipe1.set_params(data__yr_wk=yr_wk_pred)\n",
    "    #data1 = pipe1.transform(X=None)\n",
    "    #data2 = pipe2.transform(X=data1)\n",
    "    #X_y_pred = pickColumns.transform(X=data2)\n",
    "    #info_pred = infoColumns.transform(X=data2)\n",
    "    #X_pred = X_y_pred.drop(y_col, axis=1)\n",
    "    #y_pred = X_y_pred[y_col]\n",
    "    # Make prediction, just gbr for now\n",
    "    X_pred = pickColumns.fit_transform(X=pred_data).drop(y_col, axis=1)\n",
    "    \n",
    "    if(y_col == 'played'):\n",
    "        preds = gb.predict_proba(X_pred.iloc[predict_i])[:,1]\n",
    "    else:\n",
    "        preds = gb.predict(X_pred.iloc[predict_i])\n",
    "\n",
    "    df_pred[y_col] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
