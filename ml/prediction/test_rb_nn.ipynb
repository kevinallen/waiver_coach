{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import nfldb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from ml.feature_extraction.nfldb_feature_extraction import WeeklyPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import LagPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import MeanPlayerData\n",
    "from ml.feature_extraction.nfldb_feature_extraction import ExtractColumns\n",
    "from ml.feature_extraction.nfldb_feature_extraction import HandleNaN\n",
    "from ml.feature_extraction.nfldb_feature_extraction import FilterPlayedPercent\n",
    "from ml.helpers.nfldb_helpers import week_player_id_list\n",
    "from ml.helpers.nfldb_helpers import player_current_game_info\n",
    "from ml.helpers.scoring_helpers import make_scorer\n",
    "from ml.helpers.scoring_helpers import score_stats\n",
    "from ml.helpers.testing_helpers import train_test_split_index\n",
    "from ml.helpers.testing_helpers import split_by_year_week\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def position_stats(position):\n",
    "    if position == 'RB':\n",
    "        return(['receiving_rec', 'receiving_tar', 'receiving_tds', 'receiving_yac_yds', 'receiving_yds', 'rushing_att', 'rushing_tds','rushing_yds'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_feature_set(db, result_path='../results', cache_path='../data', position='RB', load_cached=True, nlag=6, to_yr_wk=(2015, 6), stat_override=None):\n",
    "    if(not load_cached):\n",
    "        # make player data transformer\n",
    "        yr_wk = [(j, i) for j in range(2009,to_yr_wk[0]) for i in range(1,18)]\n",
    "        yr_wk += [(to_yr_wk[0], i) for i in range(1,to_yr_wk[1]+1)]\n",
    "        \n",
    "        if(stat_override):\n",
    "            stats = stat_override\n",
    "        else:\n",
    "            stats = position_stats(position)\n",
    "\n",
    "        player_info = ['player_id','full_name','position']\n",
    "        playerData = WeeklyPlayerData(db=db, yr_wk=yr_wk, stats=stats, player_info=player_info, fill_time=True, position=position)\n",
    "\n",
    "        # creates lags of the data\n",
    "        lag_cols = ['year', 'week', 'played'] + stats\n",
    "        lagData = LagPlayerData(nlag=nlag, groupby_cols=['player_id'], lag_cols=lag_cols, same_year_bool=True)\n",
    "\n",
    "        # creates means of the data\n",
    "        mean_cols = stats\n",
    "        meanData = MeanPlayerData(groupby_cols=['player_id'], mean_cols=mean_cols)\n",
    "\n",
    "        # pipeline for getting data\n",
    "        pipe1 = Pipeline(steps=[('data',playerData), ('lag',lagData), ('mean',meanData)])\n",
    "        #processed_data = pipe1.fit_transform(X=None)\n",
    "\n",
    "        # print processed_data\n",
    "        # pipeline for seting which columns we want and handling NaN\n",
    "        pct_played_threshold = 0.0\n",
    "        pipe2_steps = [('handle',HandleNaN(method='fill')), ('filterplayed',FilterPlayedPercent(pct_played_threshold=pct_played_threshold))]\n",
    "        pipe2 = Pipeline(steps=pipe2_steps)\n",
    "\n",
    "        pipe = Pipeline([('pipe1',pipe1),('pipe2',pipe2)])\n",
    "\n",
    "        all_columns = pipe.fit_transform(X=None)\n",
    "        all_columns.position = all_columns.position.astype(str)\n",
    "\n",
    "        # pickle files\n",
    "        pickle.dump(pipe.set_params(pipe1__data__db=None), open(cache_path + '/pipe_'+position+'.p', 'wb'))\n",
    "        pickle.dump(all_columns, open(cache_path + '/data_'+position+'.p', 'wb'))\n",
    "    else:\n",
    "        # Load from \"cached\" (pickled) transformer and data\n",
    "        # data\n",
    "        all_columns = pickle.load(open(cache_path + '/data_'+position+'.p', 'rb'))\n",
    "        # pipeline\n",
    "        pipe = pickle.load(open(cache_path + '/pipe_'+position+'.p', 'rb'))\n",
    "        # retrieve the list of stats that was predicted\n",
    "        pipe_params = pipe.get_params()\n",
    "        stats = pipe_params['pipe1__data__stats']\n",
    "\n",
    "    pipe.set_params(pipe1__data__db=db)\n",
    "    \n",
    "    return (all_columns, pipe, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = nfldb.connect()\n",
    "full_train, pipe, stats = load_feature_set(db)\n",
    "\n",
    "# picks columns to model\n",
    "lag_cols = [stat + '_lag' for stat in stats]\n",
    "mean_cols = [stat + '_mean' for stat in stats]\n",
    "other_cols = ['same_year_lag', 'played_lag']\n",
    "\n",
    "infoColumns = ExtractColumns(like=[], exact=['year','week','time','player_id','full_name'])\n",
    "row_info = infoColumns.fit_transform(X=full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### prediction data\n",
    "# prediction pipeline\n",
    "pred_data_pipe = pipe#Pipeline(steps=[('pipe1',pipe1),('pipe2',pipe2)])\n",
    "\n",
    "# get information we need to make predictions\n",
    "season_phase, cur_year, cur_week = nfldb.current(db)\n",
    "pred_week = cur_week + 1\t\n",
    "pred_yr_wk = [(j, i) for j in range(2009,cur_year-1) for i in range(1,18)]\n",
    "pred_yr_wk += [(cur_year, i) for i in range(1,pred_week+1)]\n",
    "\n",
    "pred_data_pipe.set_params(pipe1__data__yr_wk = pred_yr_wk)\n",
    "\n",
    "player_ids = week_player_id_list(db, cur_year, pred_week, position='RB')\n",
    "#player_ids = player_ids[0:2] + player_ids[-3:-1]\n",
    "\n",
    "pred_data = pred_data_pipe.fit_transform(player_ids)\n",
    "pred_info = infoColumns.fit_transform(X=pred_data)\n",
    "\n",
    "# get extra info like team and opponent\n",
    "# should probably be put in to infoColumns transformer later\n",
    "extra_info = player_current_game_info(db, year=cur_year, week=pred_week, player_ids = list(pred_info['player_id']))\n",
    "join_on = ['player_id']\n",
    "add_on = ['team', 'opp_team', 'at_home']\n",
    "pred_info = pred_info.join(extra_info.set_index(join_on).loc[:,add_on], on=join_on)\n",
    "\n",
    "# predict for the last week\n",
    "pred_yr_wk_t = [pred_yr_wk[-1]]\n",
    "garbage_i, predict_i = split_by_year_week(pred_data, pred_yr_wk_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "played_bool = full_train['played'] == 1\n",
    "played_train = full_train[played_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "df_pred = pred_info.iloc[predict_i]\n",
    "\n",
    "# set y_col\n",
    "#y_cols = ['played', 'receiving_rec', 'receiving_tds', 'receiving_yds', 'rushing_att', 'rushing_tds','rushing_yds']\n",
    "y_col = 'receiving_yds'\n",
    "#y_cols = ['rushing_yds', 'played']\n",
    "\n",
    "played_only = True\n",
    "\n",
    "\n",
    "# Pick the right columns\n",
    "keep_like = [y_col] + lag_cols + mean_cols + other_cols\n",
    "pickColumns = ExtractColumns(like=keep_like)\n",
    "\n",
    "if(played_only and y_col != 'played'):\n",
    "    X_y = pickColumns.fit_transform(X=played_train)\n",
    "else:\n",
    "    X_y = pickColumns.fit_transform(X=full_train)\n",
    "\n",
    "# get X and y\n",
    "y = X_y[y_col]\n",
    "X = X_y.drop(y_col, axis=1)\n",
    "\n",
    "# random split train and test\n",
    "train_i, test_i = train_test_split_index(X.shape[0], test_size=0.1, seed=0)\n",
    "# set up data\n",
    "y_train = y.iloc[train_i]\n",
    "y_test = y.iloc[test_i]\n",
    "X_train = X.iloc[train_i]\n",
    "X_test = X.iloc[test_i]\n",
    "### Test Predictions\n",
    "X_pred = pickColumns.fit_transform(X=pred_data).drop(y_col, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit k nearest neighbors\n",
    "k = 100\n",
    "nn = NearestNeighbors(n_neighbors=k).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns tuple of (distances, indices of neighbors)\n",
    "# for prediction set\n",
    "distance, neighbor = nn.kneighbors(X=X_pred.iloc[predict_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[i for i in range(len(predict_i)) if pred_data.iloc[predict_i].full_name[i] == 'Adrian Peterson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check neighbors\n",
    "check_i = 5\n",
    "# check_nn is a data frame where the first row is the player\n",
    "# and the rest of the rows are the nearest neighbors\n",
    "check_nn = pred_data.iloc[predict_i].iloc[[check_i],:].append(played_train.iloc[neighbor[check_i,:]])\n",
    "#check_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_nn['StandardPoints'] = score_stats(check_nn, make_scorer(base_type='standard'))\n",
    "check_nn['PPRPoints'] = score_stats(check_nn, make_scorer(base_type='ppr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_stat = 'rushing_yds'\n",
    "plot_stat = 'StandardPoints'\n",
    "\n",
    "# the histogram of the data\n",
    "stat_X = check_nn.iloc[1:][plot_stat]\n",
    "player_name = check_nn.iloc[0]['full_name']\n",
    "n, bins, patches = plt.hist(stat_X, 25, normed=1, edgecolor='none', facecolor='grey', alpha=0.25)\n",
    "\n",
    "# get plot limits\n",
    "xmin = 0\n",
    "xmax = max(bins)*1.1\n",
    "ymin = 0\n",
    "ymax = max(n)*1.1\n",
    "\n",
    "# get bins for kernel density plot\n",
    "bins = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "# set up kernel density\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=2.5).fit(X=stat_X[:,np.newaxis])\n",
    "y_smooth = np.exp(kde.score_samples(bins[:,np.newaxis]))\n",
    "y_smooth\n",
    "\n",
    "l = plt.plot(bins, y_smooth, 'b--', linewidth=1)\n",
    "plt.xlabel(plot_stat)\n",
    "plt.ylabel('Probability')\n",
    "plt.axis([0, max(bins)*1.1, 0, max(n)*1.1])\n",
    "plt.title(player_name)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
